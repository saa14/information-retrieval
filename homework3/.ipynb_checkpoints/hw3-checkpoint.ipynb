{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2018\n",
    "\n",
    "\n",
    "# Homework 3:  Embeddings + Recommenders\n",
    "\n",
    "### 100 points [5% of your final grade]\n",
    "\n",
    "### Due: Monday, April 9 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* There are two main learning objectives: (i) implement and evaluate a pre-cursor to modern word2vec embeddings; and (ii) implement, evaluate, and improve upon traditional collaborative filtering recommenders.\n",
    "\n",
    "*Submission Instructions:* To submit your homework, rename this notebook as UIN_hw#.ipynb. For example, this homework submission would be: YourUIN_hw3.ipynb. Submit this notebook via ecampus. Your notebook should be completely self-contained, with the results visible in the notebook. \n",
    "\n",
    "*Late submission policy:* For this homework, you may use up to three of your late days, meaning that no submissions will be accepted after Thursday, April 12 at 11:59pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Word Embeddings (50 points)\n",
    "For this first part, we're going to implement a word embedding approach that is a bit simpler than word2vec. The key idea is to look at co-occurrences between center words and context words (somewhat like in word2vec) but without any pesky learning of model parameters.\n",
    "\n",
    "If you're interested in a deeper treatment of comparing count vs. learned embeddings, take a look at: [Donâ€™t count, predict! A systematic comparison of\n",
    "context-counting vs. context-predicting semantic vectors](\n",
    "http://www.aclweb.org/anthology/P14-1023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Brown Corpus\n",
    "\n",
    "The dataset for this part is the (in)famous [Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus) that is a collection of text samples from a wide range of sources, with over one million unique words. Good for us, you can find the Brown corpus in nltk. *Make sure you have already installed nltk with something like: conda install nltk*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have it locally, you can load the dataset into your notebook. You can access the words using brown.words():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.words()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dataset Pre-processing\n",
    "OK, now we need to do some basic pre-processing. For this part you should:\n",
    "\n",
    "* Remove stopwords and punctuation.\n",
    "* Make everything lowercase.\n",
    "\n",
    "Then, count how often each word occurs. We will define the 5,000 most  frequent words as your vocabulary (V). We will define the 1,000 most frequent words as our context (C). Include a print statement below to show the top-20 words after pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your Code Here...\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "def punctuationCheck(w):\n",
    "    temp_word = ''.join(char for char in w if char not in set(string.punctuation))\n",
    "    if len(temp_word) !=0:\n",
    "        return temp_word\n",
    "    else:\n",
    "        return 0\n",
    "stop_list = stopwords.words('english')\n",
    "pre_processed = [w.lower() for w in brown.words() if w.lower() not in stop_list]\n",
    "pre_processed = [punctuationCheck(w) for w in pre_processed if punctuationCheck(w) != 0] \n",
    "counts = Counter(pre_processed)\n",
    "vocabulary = [w[0] for w in counts.most_common(5000)]\n",
    "context = [w[0] for w in counts.most_common(1000)]\n",
    "print len(vocabulary)\n",
    "print len(context)\n",
    "print \"Rank  \" +\"Word  \"+\"Count\" \n",
    "rank = 1\n",
    "for token, count in counts.most_common(20):\n",
    "    print str(rank)+\"      \"+str(token)+\"   \"+str(count)\n",
    "    rank += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Building the Co-occurrence Matrix \n",
    "\n",
    "For each word in the vocabulary (w), we want to calculate how often context words from C appear in its surrounding window of size 4 (two words before and two words after).\n",
    "\n",
    "In other words, we need to define a co-occurrence matrix that has a dimension of |V|x|C| such that each cell (w,c) represents the number of times c occurs in a window around w. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your Code Here...\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "vocab_index = {}\n",
    "context_index = {}\n",
    "data_length = len(pre_processed)\n",
    "def createIndexDictionary(words):\n",
    "    word_dict = {}\n",
    "    for i,w in enumerate(words):\n",
    "        word_dict[w] = i\n",
    "    return word_dict\n",
    "vocab_index = createIndexDictionary(vocabulary)\n",
    "context_index = createIndexDictionary(context) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createMatrix(words,vocab_index,context_index,window = 2):\n",
    "    coo_matrix = np.zeros((len(vocabulary),len(context)))\n",
    "    for i in range(data_length):\n",
    "        w = words[i]\n",
    "        if w in vocab_index:\n",
    "            context_window = np.arange(max(i - window,0),min(i + window + 1,data_length))\n",
    "            context_window = np.setdiff1d(context_window,np.array([1]))\n",
    "            for j in context_window:\n",
    "                c = words[j]\n",
    "                if c in context_index:\n",
    "                    coo_matrix[vocab_index[w]][context_index[c]] += 1             \n",
    "    return coo_matrix\n",
    "coo_matrix = createMatrix(pre_processed,vocab_index,context_index)\n",
    "                \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Probability Distribution\n",
    "\n",
    "Using the co-occurrence matrix, we can compute the probability distribution Pr(c|w) of context word c around w as well as the overall probability distribution of each context word c with Pr(c).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your Code Here...\n",
    "\n",
    "# print coo_matrix.shape\n",
    "\n",
    "def getProbDist(coo_matrix):\n",
    "    prob_matrix = np.divide(coo_matrix.T,coo_matrix.sum(axis = 1))\n",
    "    return prob_matrix\n",
    "\n",
    "prob_matrix = getProbDist(coo_matrix)\n",
    "# print prob_matrix.shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getOverallProb(coo_matrix):\n",
    "    \n",
    "    overall_counts = ((coo_matrix).sum(axis = 0))\n",
    "    return np.divide(overall_counts,overall_counts.sum())\n",
    "\n",
    "overall_probabilities = getOverallProb(coo_matrix)\n",
    "# print overall_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Embedding Representation\n",
    "\n",
    "Now you can represent each vocabulary word as a |C| dimensional vector using this equation:\n",
    "\n",
    "Vector(w)= max(0, log (Pr(c|w)/Pr(c)))\n",
    "\n",
    "This is a traditional approach called *pointwise mutual information* that pre-dates word2vec by some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your Code Here...\n",
    "def getPMIMatrix(prob_matrix,overall_probabilities):\n",
    "    pmi_matrix =  np.divide(prob_matrix.T,overall_probabilities)\n",
    "    pmi_matrix = np.maximum(np.zeros((len(vocabulary), len(context))), np.log(pmi_matrix))\n",
    "    print pmi_matrix.shape\n",
    "    return pmi_matrix\n",
    "pmi_matrix = getPMIMatrix(prob_matrix,overall_probabilities)\n",
    "%store pmi_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Analysis\n",
    "\n",
    "So now we have some embeddings for each word. But are they meaningful? For this part, you should:\n",
    "\n",
    "- First, cluster the vocabulary into 100 clusters using k-means. Look over the words in each cluster, can you see any relation beween words? Discuss your observations.\n",
    "\n",
    "- Second, for the top-20 most frequent words, find the nearest neighbors using cosine distance (1- cosine similarity). Do the findings make sense? Discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First part\n",
    "%store -r pmi_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans_model = KMeans(n_clusters=100,max_iter=300)\n",
    "kmeans_model.fit(pmi_matrix)\n",
    "centroids = kmeans_model.cluster_centers_.argsort(axis=1)[:,::-1]\n",
    "reverse_vocab_index = {w:i for i,w in vocab_index.items()}\n",
    "for i in range(10):\n",
    "    print \"\\n****** Cluster - \"+ str(i+1) + \" ******\"\n",
    "    for c in centroids[i, :10]:\n",
    "        print reverse_vocab_index[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Second part\n",
    "%store -r pmi_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nn_model = NearestNeighbors(n_neighbors=10, metric = 'cosine')\n",
    "nn_model.fit(pmi_matrix)\n",
    "nearest_neighbours = nn_model.kneighbors(pmi_matrix[:20,], n_neighbors=10)\n",
    "nn_index = nearest_neighbours[1]\n",
    "reverse_context_index = {w:i for i,w in context_index.items()}\n",
    "for i in range(20):\n",
    "    print \"\\n****** Word - \"+str(reverse_vocab_index.get(i))+\" ******\"\n",
    "    for n in nn_index[i, :]:\n",
    "        print reverse_context_index.get(n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Collaborative Filtering (50 points)\n",
    "\n",
    "In this second part, you will implement collaborative filtering on the Netflix prize dataset -- donâ€™t freak out, the provided sample dataset has only ~2000 items and ~28,000 users.\n",
    "\n",
    "As background, read the paper [Empirical Analysis of Predictive Algorithms for Collaborative Filtering](https://arxiv.org/pdf/1301.7363.pdf) up to Section 2.1. Of course you can read further if you are interested, and you can also refer to the course slides for collaborative filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load Netflix Data\n",
    "\n",
    "The dataset is subset of movie ratings data from the Netflix Prize Challenge. Download the dataset from Piazza. It contains a train set, test set, movie file, and README file. The last two files are original ones from the Netflix Prize, however; in this homework you will deal with train and test files which both are subsets of the Netflix training data. Each of train and test files has lines having this format: MovieID,UserID,Rating.\n",
    "\n",
    "Your job is to predict a rating in the test set using those provided in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "28978 Users\n",
      "1821 Movies\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>1744889</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>1395430</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>1205593</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>1488844</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>1447354</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>306466</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>1331154</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1818178</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>991725</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>1987434</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>1765381</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8</td>\n",
       "      <td>433803</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8</td>\n",
       "      <td>1148143</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8</td>\n",
       "      <td>1174811</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8</td>\n",
       "      <td>1684516</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>754781</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>567025</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>1623132</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>1567095</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8</td>\n",
       "      <td>1666394</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8</td>\n",
       "      <td>622194</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8</td>\n",
       "      <td>966375</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8</td>\n",
       "      <td>116582</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8</td>\n",
       "      <td>1646405</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8</td>\n",
       "      <td>174868</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8</td>\n",
       "      <td>327122</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8</td>\n",
       "      <td>135348</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8</td>\n",
       "      <td>944189</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8</td>\n",
       "      <td>1041552</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8</td>\n",
       "      <td>2459251</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255322</th>\n",
       "      <td>17742</td>\n",
       "      <td>204675</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255323</th>\n",
       "      <td>17742</td>\n",
       "      <td>519020</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255324</th>\n",
       "      <td>17742</td>\n",
       "      <td>387819</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255325</th>\n",
       "      <td>17742</td>\n",
       "      <td>680004</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255326</th>\n",
       "      <td>17742</td>\n",
       "      <td>1639792</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255327</th>\n",
       "      <td>17742</td>\n",
       "      <td>2224734</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255328</th>\n",
       "      <td>17742</td>\n",
       "      <td>2631660</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255329</th>\n",
       "      <td>17742</td>\n",
       "      <td>2157611</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255330</th>\n",
       "      <td>17742</td>\n",
       "      <td>2339135</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255331</th>\n",
       "      <td>17742</td>\n",
       "      <td>2500594</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255332</th>\n",
       "      <td>17742</td>\n",
       "      <td>2385226</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255333</th>\n",
       "      <td>17742</td>\n",
       "      <td>734347</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255334</th>\n",
       "      <td>17742</td>\n",
       "      <td>1930483</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255335</th>\n",
       "      <td>17742</td>\n",
       "      <td>143938</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255336</th>\n",
       "      <td>17742</td>\n",
       "      <td>1816485</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255337</th>\n",
       "      <td>17742</td>\n",
       "      <td>2232581</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255338</th>\n",
       "      <td>17742</td>\n",
       "      <td>391708</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255339</th>\n",
       "      <td>17742</td>\n",
       "      <td>1326195</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255340</th>\n",
       "      <td>17742</td>\n",
       "      <td>2516156</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255341</th>\n",
       "      <td>17742</td>\n",
       "      <td>775020</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255342</th>\n",
       "      <td>17742</td>\n",
       "      <td>859167</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255343</th>\n",
       "      <td>17742</td>\n",
       "      <td>88188</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255344</th>\n",
       "      <td>17742</td>\n",
       "      <td>391521</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255345</th>\n",
       "      <td>17742</td>\n",
       "      <td>517335</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255346</th>\n",
       "      <td>17742</td>\n",
       "      <td>98676</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255347</th>\n",
       "      <td>17742</td>\n",
       "      <td>46222</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255348</th>\n",
       "      <td>17742</td>\n",
       "      <td>2534701</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255349</th>\n",
       "      <td>17742</td>\n",
       "      <td>208724</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255350</th>\n",
       "      <td>17742</td>\n",
       "      <td>483107</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255351</th>\n",
       "      <td>17742</td>\n",
       "      <td>1181331</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3255352 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         MovieID   UserID  Rating\n",
       "0              8  1744889     1.0\n",
       "1              8  1395430     2.0\n",
       "2              8  1205593     4.0\n",
       "3              8  1488844     4.0\n",
       "4              8  1447354     1.0\n",
       "5              8   306466     4.0\n",
       "6              8  1331154     4.0\n",
       "7              8  1818178     3.0\n",
       "8              8   991725     4.0\n",
       "9              8  1987434     4.0\n",
       "10             8  1765381     4.0\n",
       "11             8   433803     3.0\n",
       "12             8  1148143     2.0\n",
       "13             8  1174811     5.0\n",
       "14             8  1684516     3.0\n",
       "15             8   754781     4.0\n",
       "16             8   567025     4.0\n",
       "17             8  1623132     4.0\n",
       "18             8  1567095     3.0\n",
       "19             8  1666394     5.0\n",
       "20             8   622194     4.0\n",
       "21             8   966375     4.0\n",
       "22             8   116582     1.0\n",
       "23             8  1646405     5.0\n",
       "24             8   174868     2.0\n",
       "25             8   327122     4.0\n",
       "26             8   135348     2.0\n",
       "27             8   944189     4.0\n",
       "28             8  1041552     3.0\n",
       "29             8  2459251     4.0\n",
       "...          ...      ...     ...\n",
       "3255322    17742   204675     3.0\n",
       "3255323    17742   519020     2.0\n",
       "3255324    17742   387819     3.0\n",
       "3255325    17742   680004     2.0\n",
       "3255326    17742  1639792     1.0\n",
       "3255327    17742  2224734     4.0\n",
       "3255328    17742  2631660     3.0\n",
       "3255329    17742  2157611     2.0\n",
       "3255330    17742  2339135     2.0\n",
       "3255331    17742  2500594     4.0\n",
       "3255332    17742  2385226     2.0\n",
       "3255333    17742   734347     2.0\n",
       "3255334    17742  1930483     2.0\n",
       "3255335    17742   143938     3.0\n",
       "3255336    17742  1816485     2.0\n",
       "3255337    17742  2232581     3.0\n",
       "3255338    17742   391708     3.0\n",
       "3255339    17742  1326195     1.0\n",
       "3255340    17742  2516156     2.0\n",
       "3255341    17742   775020     3.0\n",
       "3255342    17742   859167     1.0\n",
       "3255343    17742    88188     2.0\n",
       "3255344    17742   391521     3.0\n",
       "3255345    17742   517335     2.0\n",
       "3255346    17742    98676     3.0\n",
       "3255347    17742    46222     3.0\n",
       "3255348    17742  2534701     1.0\n",
       "3255349    17742   208724     3.0\n",
       "3255350    17742   483107     2.0\n",
       "3255351    17742  1181331     2.0\n",
       "\n",
       "[3255352 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Data\n",
      "27555 Users\n",
      "1701 Movies\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>573364</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>2149668</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>1089184</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>2465894</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>534508</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>992921</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>595054</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1298304</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1661600</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>553787</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>1309839</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8</td>\n",
       "      <td>727242</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8</td>\n",
       "      <td>1437668</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8</td>\n",
       "      <td>2170930</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8</td>\n",
       "      <td>1780876</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>9660</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>2379200</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>563186</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>1539617</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8</td>\n",
       "      <td>1656839</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8</td>\n",
       "      <td>2591126</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8</td>\n",
       "      <td>2455107</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8</td>\n",
       "      <td>1079943</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8</td>\n",
       "      <td>2308589</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8</td>\n",
       "      <td>254671</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8</td>\n",
       "      <td>2633545</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8</td>\n",
       "      <td>1589111</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8</td>\n",
       "      <td>320540</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8</td>\n",
       "      <td>1769901</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8</td>\n",
       "      <td>155279</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100448</th>\n",
       "      <td>17741</td>\n",
       "      <td>1428560</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100449</th>\n",
       "      <td>17741</td>\n",
       "      <td>2426395</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100450</th>\n",
       "      <td>17741</td>\n",
       "      <td>977681</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100451</th>\n",
       "      <td>17741</td>\n",
       "      <td>795380</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100452</th>\n",
       "      <td>17741</td>\n",
       "      <td>2078245</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100453</th>\n",
       "      <td>17741</td>\n",
       "      <td>1978379</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100454</th>\n",
       "      <td>17741</td>\n",
       "      <td>85824</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100455</th>\n",
       "      <td>17741</td>\n",
       "      <td>274652</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100456</th>\n",
       "      <td>17741</td>\n",
       "      <td>406948</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100457</th>\n",
       "      <td>17741</td>\n",
       "      <td>1198632</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100458</th>\n",
       "      <td>17741</td>\n",
       "      <td>1126516</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100459</th>\n",
       "      <td>17741</td>\n",
       "      <td>2611783</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100460</th>\n",
       "      <td>17741</td>\n",
       "      <td>285164</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100461</th>\n",
       "      <td>17741</td>\n",
       "      <td>1061195</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100462</th>\n",
       "      <td>17741</td>\n",
       "      <td>970206</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100463</th>\n",
       "      <td>17741</td>\n",
       "      <td>326502</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100464</th>\n",
       "      <td>17741</td>\n",
       "      <td>2487973</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100465</th>\n",
       "      <td>17741</td>\n",
       "      <td>2560910</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100466</th>\n",
       "      <td>17741</td>\n",
       "      <td>2072547</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100467</th>\n",
       "      <td>17741</td>\n",
       "      <td>772989</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100468</th>\n",
       "      <td>17741</td>\n",
       "      <td>2419258</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100469</th>\n",
       "      <td>17741</td>\n",
       "      <td>953604</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100470</th>\n",
       "      <td>17742</td>\n",
       "      <td>1363333</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100471</th>\n",
       "      <td>17742</td>\n",
       "      <td>1843135</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100472</th>\n",
       "      <td>17742</td>\n",
       "      <td>2636654</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100473</th>\n",
       "      <td>17742</td>\n",
       "      <td>1898310</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100474</th>\n",
       "      <td>17742</td>\n",
       "      <td>716096</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100475</th>\n",
       "      <td>17742</td>\n",
       "      <td>38115</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100476</th>\n",
       "      <td>17742</td>\n",
       "      <td>2646347</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100477</th>\n",
       "      <td>17742</td>\n",
       "      <td>273576</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100478 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        MovieID   UserID  Rating\n",
       "0             8   573364     1.0\n",
       "1             8  2149668     3.0\n",
       "2             8  1089184     3.0\n",
       "3             8  2465894     3.0\n",
       "4             8   534508     1.0\n",
       "5             8   992921     4.0\n",
       "6             8   595054     4.0\n",
       "7             8  1298304     4.0\n",
       "8             8  1661600     4.0\n",
       "9             8   553787     2.0\n",
       "10            8  1309839     3.0\n",
       "11            8   727242     1.0\n",
       "12            8  1437668     4.0\n",
       "13            8  2170930     1.0\n",
       "14            8  1780876     5.0\n",
       "15            8     9660     3.0\n",
       "16            8  2379200     4.0\n",
       "17            8   563186     5.0\n",
       "18            8  1539617     4.0\n",
       "19            8  1656839     1.0\n",
       "20            8  2591126     4.0\n",
       "21            8  2455107     3.0\n",
       "22            8  1079943     5.0\n",
       "23            8  2308589     4.0\n",
       "24            8   254671     4.0\n",
       "25            8  2633545     3.0\n",
       "26            8  1589111     2.0\n",
       "27            8   320540     4.0\n",
       "28            8  1769901     3.0\n",
       "29            8   155279     3.0\n",
       "...         ...      ...     ...\n",
       "100448    17741  1428560     3.0\n",
       "100449    17741  2426395     3.0\n",
       "100450    17741   977681     2.0\n",
       "100451    17741   795380     5.0\n",
       "100452    17741  2078245     5.0\n",
       "100453    17741  1978379     3.0\n",
       "100454    17741    85824     2.0\n",
       "100455    17741   274652     2.0\n",
       "100456    17741   406948     4.0\n",
       "100457    17741  1198632     3.0\n",
       "100458    17741  1126516     4.0\n",
       "100459    17741  2611783     2.0\n",
       "100460    17741   285164     2.0\n",
       "100461    17741  1061195     5.0\n",
       "100462    17741   970206     4.0\n",
       "100463    17741   326502     3.0\n",
       "100464    17741  2487973     1.0\n",
       "100465    17741  2560910     4.0\n",
       "100466    17741  2072547     2.0\n",
       "100467    17741   772989     1.0\n",
       "100468    17741  2419258     2.0\n",
       "100469    17741   953604     2.0\n",
       "100470    17742  1363333     3.0\n",
       "100471    17742  1843135     2.0\n",
       "100472    17742  2636654     4.0\n",
       "100473    17742  1898310     2.0\n",
       "100474    17742   716096     4.0\n",
       "100475    17742    38115     3.0\n",
       "100476    17742  2646347     5.0\n",
       "100477    17742   273576     2.0\n",
       "\n",
       "[100478 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the data, then print out the number of ratings, \n",
    "# movies and users in each of train and test sets.\n",
    "# Your Code Here...\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "training_data = pd.read_csv('netflix-dataset/TrainingRatings.txt', sep=\",\", names = ['MovieID','UserID','Rating'])\n",
    "testing_data = pd.read_csv('netflix-dataset/TestingRatings.txt', sep=\",\", names = ['MovieID','UserID','Rating'])\n",
    "print \"Training Data\"\n",
    "training_users = training_data.UserID.unique()\n",
    "training_movies = training_data.MovieID.unique()\n",
    "tr_users = training_users.shape[0]\n",
    "tr_movies = training_movies.shape[0]\n",
    "print str(tr_users)+\" Users\"\n",
    "print str(tr_movies)+\" Movies\"\n",
    "display(training_data)\n",
    "training_data.to_csv('training.csv')\n",
    "print \"Testing Data\"\n",
    "testing_users = testing_data.UserID.unique()\n",
    "testing_movies = testing_data.MovieID.unique()\n",
    "te_users = testing_users.shape[0]\n",
    "te_movies = testing_movies.shape[0]\n",
    "print str(testing_data.UserID.unique().shape[0])+\" Users\"\n",
    "print str(testing_data.MovieID.unique().shape[0])+\" Movies\"\n",
    "display(testing_data)\n",
    "testing_data.to_csv('testing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Implement CF\n",
    "\n",
    "In this part, you will implement the basic collaborative filtering algorithm described in Section 2.1 of the paper -- that is, focus only on Equations 1 and 2 (where Equation 2 is just the Pearson correlation). You should consider the first 5,000 users with their associated items in the test set. \n",
    "\n",
    "Note that you should test the algorithm for a small set of users e.g., 10 users first and then run for 5,000 users. It may take long to run but you won't have memory issues. \n",
    "\n",
    "Set k to 0.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "cpdef getMoviesForUser(user,data):\n",
    "    check_user = data['UserID'] == user\n",
    "    return (data[check_user]['MovieID']).tolist()\n",
    "cpdef pearsonCorr(u1_ratings, u2_ratings):\n",
    "    return pearsonr(u1_ratings,u2_ratings)[0]\n",
    "def createCorrelationMatrix(testing_users, training_users,tr_user_index,te_movie_index,te_user_index,tr_movie_index,testing_matrix,training_matrix,training_data,testing_data):\n",
    "    corr_matrix = np.zeros((5000,len(tr_user_index)),dtype=np.dtype(\"f\"))\n",
    "    cdef float [:,:] corr_view = corr_matrix    \n",
    "    cdef u1_ratings = []\n",
    "    cdef u2_ratings = []\n",
    "    cdef common_movies = []\n",
    "    cdef te_users = testing_users[:5000]\n",
    "    len_te = len(te_users)\n",
    "    cdef tr_users = training_users\n",
    "    len_tr = len(training_users)\n",
    "    for index1 in te_users[:len_te]:\n",
    "        u1_movies = getMoviesForUser(te_users[index1],testing_data)\n",
    "        u1i = te_user_index.get(te_users[index1])\n",
    "        for index2 in tr_users[:len_tr]:\n",
    "            u2i = tr_user_index.get(tr_users[index2])\n",
    "            u2_movies = getMoviesForUser(te_users[index2],training_data)\n",
    "            common_movies = np.intersect1d(u1_movies,u2_movies)\n",
    "            len_c = len(common_movies)\n",
    "            for index3 in common_movies[:len_c]:\n",
    "                m1i = te_movie_index.get(common_movies[index3])\n",
    "                m2i = tr_movie_index.get(common_movies[index3])\n",
    "                u1_ratings.append(testing_matrix[u1i,m1i])\n",
    "                u2_ratings.append(training_matrix[u2i,m2i])\n",
    "            corr_view[u1i,u2i] = pearsonCorr(u1_ratings,u2_ratings)\n",
    "    return corr_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def createNetflixIndexDictionary(items):\n",
    "    index_dict = {}\n",
    "    for i,item in enumerate(items):\n",
    "        index_dict[item] = i\n",
    "    return index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_user_index = createNetflixIndexDictionary(training_users)\n",
    "tr_movie_index = createNetflixIndexDictionary(training_movies) \n",
    "te_user_index = createNetflixIndexDictionary(testing_users)\n",
    "te_movie_index = createNetflixIndexDictionary(testing_movies) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createRatingMatrix(data,user_index,movie_index):\n",
    "    temp_matrix = np.zeros((len(user_index),len(movie_index))) \n",
    "    for record in data.itertuples():\n",
    "        ui = user_index.get(record[2])\n",
    "        mi = movie_index.get(record[1])\n",
    "        temp_matrix.itemset((ui,mi),record[3])\n",
    "    return temp_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createUserMovieDictionary(user_index,movie_index,data_matrix):\n",
    "    user_movies_dict = {}\n",
    "    for user in user_index:\n",
    "        ui = user_index.get(user)\n",
    "        movies = []\n",
    "        for movie in movie_index:\n",
    "            mi = movie_index.get(movie)\n",
    "            if data_matrix[ui,mi] != 0.0:\n",
    "                movies.append(movie)\n",
    "        user_movies_dict[user] = movies\n",
    "    return user_movies_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_matrix = createRatingMatrix(training_data,tr_user_index,tr_movie_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_matrix = createRatingMatrix(testing_data,te_user_index,te_movie_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_user_movie = createUserMovieDictionary(tr_user_index,tr_movie_index,training_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "te_user_movie = createUserMovieDictionary(te_user_index,te_movie_index,testing_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.stats import pearsonr\n",
    "def calculateAvgRating(user,user_index,data_matrix):\n",
    "    ui = user_index.get(user)\n",
    "    return np.mean(data_matrix[ui])\n",
    "def createAvgRatingDictionary(users,user_index,matrix):\n",
    "    avg_dict = {}\n",
    "    for user in users:\n",
    "        avg_dict[user] = calculateAvgRating(user,user_index,matrix)\n",
    "    return avg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_avg_dict = createAvgRatingDictionary(training_users,tr_user_index,training_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "te_avg_dict = createAvgRatingDictionary(testing_users,te_user_index,testing_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createCorrelationDictonary(no_of_users):\n",
    "    corr_dict = {}\n",
    "    num_rating = 0\n",
    "    denom_sum1 = 0\n",
    "    denom_sum2 = 0\n",
    "    for user1 in testing_users[:no_of_users]:\n",
    "#         u1_ratings = []\n",
    "#         u2_ratings = []\n",
    "        corr_dict[user1] = {}\n",
    "        for user2 in training_users:\n",
    "            if user1 != user2:\n",
    "                common_movies = np.intersect1d(te_user_movie[user1],tr_user_movie[user2])\n",
    "                if common_movies.size != 0:\n",
    "                    for movie in common_movies:\n",
    "                        m1i = te_movie_index.get(movie)\n",
    "                        m2i = tr_movie_index.get(movie)\n",
    "                        u1i = te_user_index.get(user1)\n",
    "                        u2i = tr_user_index.get(user2)\n",
    "                        num_rating += ((testing_matrix[u1i,m1i] - te_avg_dict[user1])*(training_matrix[u2i,m2i] - tr_avg_dict[user2]))\n",
    "                        denom_sum1 += (testing_matrix[u1i,m1i] - te_avg_dict[user1])**2\n",
    "                        denom_sum2 += (training_matrix[u2i,m2i] - tr_avg_dict[user2])**2\n",
    "#                         u1_ratings.append(testing_matrix[u1i,m1i])\n",
    "#                         u2_ratings.append(training_matrix[u2i,m2i])\n",
    "#                     corr_dict[user1][user2] = np.corrcoef(u1_ratings,u2_ratings)\n",
    "                    corr_dict[user1][user2] = num_rating/math.sqrt(float(denom_sum1*denom_sum2)) \n",
    "    return corr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_dict = createCorrelationDictonary(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "                \n",
    "# def createCorrelationMatrix():\n",
    "#     corr_matrix = np.zeros((5000,len(tr_user_index)))\n",
    "#     u1_ratings = []\n",
    "#     u2_ratings = []\n",
    "#     for user1 in testing_users[:5000]:\n",
    "#         u1_movies = getMoviesForUser(user1,testing_data).tolist()\n",
    "#         u1i = te_user_index.get(user1)\n",
    "#         for user2 in training_users:\n",
    "#             u2i = tr_user_index.get(user2)\n",
    "#             u2_movies = getMoviesForUser(user2,training_data).tolist()\n",
    "#             common_movies = np.intersect1d(u1_movies,u2_movies)\n",
    "#             for movie in common_movies:\n",
    "#                 m1i = te_movie_index.get(movie)\n",
    "#                 m2i = tr_movie_index.get(movie)\n",
    "#                 u1_ratings.append(testing_matrix[u1i,m1i])\n",
    "#                 u2_ratings.append(training_matrix[u2i,m2i])\n",
    "#             corr_matrix[u1i,u2i] = pearsonr(u1_ratings,u2_ratings)[0]\n",
    "#     return corr_matrix\n",
    "# def getPearsonDict(user1,movie):\n",
    "#     pearson_dict = {}\n",
    "#     u1_movies = te_user_movie[user1]\n",
    "#     for user2 in tr_user_index:\n",
    "#         u2_movies = tr_user_movie[user2]\n",
    "#         if user1 != user2 and movie in u2_movies:\n",
    "#             pc = pearsonCoefficient(user1, user2, u1_movies, u2_movies)\n",
    "#             if (not np.isnan(pc)):\n",
    "#                 pearson_dict.setdefault(user2,pc)\n",
    "#     return  pearson_dict\n",
    "def calculateK(movie,active_user):\n",
    "    check_movie = training_data['MovieID'] == movie\n",
    "    user_list = (training_data[check_movie]['UserID'])\n",
    "    sum_p = 0.0\n",
    "    k = 0.0\n",
    "    for user in user_list:\n",
    "        if (not (pearson_dict.get(active_user) == None)):\n",
    "            sum_p += abs(pearson_dict[active_user][user])\n",
    "    if sum_p == 0.0:\n",
    "        k = 0.0\n",
    "    else:\n",
    "        k = 1/float(sum_p)    \n",
    "    return k\n",
    "def predictRating(active_user,movie):\n",
    "    active_avg = calculateAvgRating(active_user,te_user_index,testing_matrix)\n",
    "    sum_rating = 0.0\n",
    "    mi = tr_movie_index.get(movie)\n",
    "    for user in tr_user_index:\n",
    "        ui =  tr_user_index.get(user)\n",
    "        if (pearson_dict.get(active_user) != None and pearson_dict.get(active_user).get(user) != None):\n",
    "            pearson_c = pearson_dict.get(active_user).get(user)\n",
    "            print pearson_c\n",
    "            sum_rating += (pearson_c)*(training_matrix[ui,mi] - tr_avg_dict[user])\n",
    "#     k = calculateK(movie,active_user)\n",
    "    k = 0.0001\n",
    "    prediction  = active_avg + k*(sum_rating)\n",
    "    if prediction > 5.0:\n",
    "        prediction = 5.0\n",
    "    elif prediction < 1.0:\n",
    "        prediction = 1.0\n",
    "    return float(int(round(prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictionForTestSet(number_of_users):\n",
    "    test_predictions={}\n",
    "    for user in testing_users[:number_of_users]:\n",
    "        user_movies = te_user_movie[user]\n",
    "        user_predictions = []\n",
    "        for movie in user_movies:\n",
    "            user_predictions.append(predictRating(user,movie))\n",
    "        test_predictions[user] = user_predictions\n",
    "    return test_predictions\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = predictionForTestSet(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print predictions[2149668]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Evaluation \n",
    "\n",
    "You should evaluate your predictions using Mean Absolute Error and Root Mean Squared Error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def getGroundTruth(number_of_users):\n",
    "    test_gt={}\n",
    "    for user in testing_users[:number_of_users]:\n",
    "        ui = te_user_index.get(user)\n",
    "        user_movies = te_user_movie[user]\n",
    "        user_gt = []\n",
    "        for movie in user_movies:\n",
    "            mi = te_movie_index.get(movie)\n",
    "            user_gt.append(testing_matrix[ui,mi])\n",
    "        test_gt[user] = user_gt\n",
    "    return test_gt\n",
    "def meanAbsoluteError(number_of_users,test_predictions,test_gt):\n",
    "    mae_list = []\n",
    "    for user in test_predictions:\n",
    "        if user in test_gt:\n",
    "            mae = mean_absolute_error(test_gt.get(user),test_predictions.get(user))\n",
    "            mae_list.append(mae)\n",
    "    return mae_list\n",
    "def rootMeanSquaredError(number_of_users,test_predictions,test_gt):\n",
    "    rmse_list = []\n",
    "    for user in test_predictions:\n",
    "        if user in test_gt:\n",
    "            rmse = math.sqrt(mean_squared_error(test_gt.get(user),test_predictions.get(user)))\n",
    "            rmse_list.append(rmse)\n",
    "    return rmse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_gt = getGroundTruth(5000)\n",
    "mae_list = meanAbsoluteError(5000,get_predictions,test_gt)\n",
    "rmse_list = rootMeanSquaredError(5000,get_predictions,test_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print mae_list\n",
    "print rmse_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Extensions\n",
    "\n",
    "Given your results in the previous part, can you do better? For this last part you should report on your best attempt at improving MAE and RMSE. Provide code, results, plus a brief discussion on your approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your Code Here...\n",
    "#Default Values\n",
    "\n",
    "\n",
    "#Inverse User Frequencies\n",
    "\n",
    "#Different Weighing Strategies\n",
    "\n",
    "#Spearman Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Insert discussion here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
