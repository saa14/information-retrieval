{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2018\n",
    "\n",
    "\n",
    "# Homework 1:  Basic Machine Learning + Learning to Rank \n",
    "\n",
    "### 100 points [5% of your final grade]\n",
    "\n",
    "### Due: Monday, February 12 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* In this homework you will get hands-on experience with (i) the basics of machine learning (e.g. train/test data, cross-validation, different classifiers) and interpreting results; and (ii) learning to rank.\n",
    "\n",
    "*Submission Instructions:* To submit your homework, rename this notebook as UIN_hw#.ipynb. For example, this homework submission would be: YourUIN_hw1.ipynb. Submit this notebook via ecampus. Your notebook should be completely self-contained, with the results visible in the notebook. \n",
    "\n",
    "*Late submission policy:* For this homework, you may use up to three of your late days, meaning that no submissions will be accepted after Thursday, February 15 at 11:59pm.\n",
    "\n",
    "*Collaboration policy:* You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Piazza, search StackOverflow, etc. But if you do get help in this way, you must inform us by **filling out the Collaboration Declarations at the bottom of this notebook**. \n",
    "\n",
    "*Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2.*\n",
    "\n",
    "The basic rule is that no student should explicitly share a solution with another student (and thereby circumvent the basic learning process), but it is okay to share general approaches, directions, and so on. If you feel like you have an issue that needs clarification, feel free to contact either me or the TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Basics of ML (70 points)\n",
    "\n",
    "For this part, we're going to get familiar with scikit-learn (a great ML toolkit that is very popular) and the major issues in training a model, testing it, and interpreting the results. Our goal in this assignment is to build a classifier to determine if a Yelp review is \"food-relevant\" or not.\n",
    "\n",
    "## Dataset: Yelp review data\n",
    "\n",
    "First, you will need to download the training_data.json file from the Resources tab on Piazza, a collection of 40,000 json-encoded Yelp reviews we sampled from the [Yelp Dataset Challenge](https://www.yelp.com/dataset_challenge).\n",
    "\n",
    "You'll see that each line corresponds to a review on a particular business. The label (class) information of each review is in the \"label\" field. It is **either \"Food-relevant\" or \"Food-irrelevant\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.1: Parsing Yelp (15 points)\n",
    "\n",
    "For this first part, we will build a parser for extracting tokens from the **review text** only. First, you should tokenize each review using **whitespaces and punctuations as delimiters**. Do not remove stopwords. You should apply casefolding (lower case everything) and use the [nltk Porter stemmer](http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.porter) ... you may need to install nltk if you don't have it already. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "# use as many cells as you need\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "# nltk.download('punkt')\n",
    "data = pd.read_json('training_data.json', lines = True)\n",
    "def yelp_parser(review):\n",
    "    stemmer = PorterStemmer()\n",
    "    review = ''.join(char for char in review if char not in set(string.punctuation))\n",
    "    review = review.lower().split()\n",
    "#     print len(review)\n",
    "    token_list = [stemmer.stem(word).encode('ascii','ignore') for word in review]\n",
    "#     print token_list\n",
    "#     print len(token_list)\n",
    "#     token_list = set(token_list)\n",
    "#     print len(token_list)\n",
    "    return token_list\n",
    "def yelp_reviews_parser():\n",
    "    reviews = data[\"text\"]\n",
    "    token_lists = []\n",
    "    for r in reviews:\n",
    "#         print r\n",
    "        token_list = yelp_parser(r)\n",
    "#         token_list = word_tokenize(r)\n",
    "        token_lists.append(token_list)\n",
    "#         print token_list\n",
    "#         print len(token_list)\n",
    "#     print len(token_lists)\n",
    "    return token_lists\n",
    "parsed_data = yelp_reviews_parser()    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique tokens?\n",
    "\n",
    "Once you have your parser working, you should report here the size of your feature space. That is, how many unique tokens do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens- 5009780\n",
      "Feature Space Size (unique tokens)- 57334\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import itertools\n",
    "from itertools import chain\n",
    "temp_list = list(itertools.chain.from_iterable(parsed_data))\n",
    "print \"Total tokens- \" + str(len(temp_list))\n",
    "unique_tokens = set(temp_list)\n",
    "print \"Feature Space Size (unique tokens)- \" + str(len(unique_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Most Popular Words\n",
    "\n",
    "Great, now we can tokenize the documents. Let's make a list of the most popular words in our reviews. For this step, you should maintain a count of how many times each word occurs. Then you should print out the top-20 words in your reviews.\n",
    "\n",
    "Your output should look like this:\n",
    "\n",
    "Rank Token Count\n",
    "\n",
    "1 awesome 78\n",
    "\n",
    "... ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank  Token  Count\n",
      "1      the   245493\n",
      "2      and   167505\n",
      "3      i   147193\n",
      "4      a   134001\n",
      "5      to   127542\n",
      "6      it   77943\n",
      "7      of   76007\n",
      "8      wa   73925\n",
      "9      is   63418\n",
      "10      for   60732\n",
      "11      in   59622\n",
      "12      that   50590\n",
      "13      my   50439\n",
      "14      they   41132\n",
      "15      you   40565\n",
      "16      thi   39699\n",
      "17      with   39271\n",
      "18      have   39030\n",
      "19      but   37235\n",
      "20      on   35061\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import collections\n",
    "from collections import Counter\n",
    "counts = Counter(temp_list)\n",
    "print \"Rank  \" +\"Token  \"+\"Count\" \n",
    "i = 1\n",
    "for token, count in counts.most_common(20):\n",
    "    print str(i)+\"      \"+str(token)+\"   \"+str(count)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zipf's Law\n",
    "\n",
    "Recall in class our discussion of Zipf's law. Let's see if this law applies to our Yelp reviews. You should use matplotlib to plot the log-base10 term counts on the y-axis versus the log-base10 rank on the x-axis. Your aim is to create a figure like the one in Figure 5.2 of the textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,u'log10cf')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEOCAYAAACetPCkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3jY/PShnJQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc660592e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code here\n",
    "%matplotlib inline  \n",
    "\n",
    "import matplotlib.pyplot as plt  \n",
    "rank_plot = []\n",
    "count_plot = []\n",
    "i = 1\n",
    "for token, count in counts.most_common():\n",
    "    rank_plot.append(i)\n",
    "    count_plot.append(count)\n",
    "    i += 1\n",
    "x = rank_plot  \n",
    "y = count_plot\n",
    "plt.plot(x, y)\n",
    "plt.xscale('log', basex=10)\n",
    "plt.yscale('log', basey=10)\n",
    "plt.xlabel('log10rank')\n",
    "plt.ylabel('log10cf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you observe? Is this consistent with Zipf's law?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this graph, the frequency(log-base10 counts) is plotted as a function of frequency rank(log-base10 ranks) for the terms in the reviews. Yes, the Zipf's law holds for this collection of terms as the collection frequency of $i^{th}$ most common term is more or less proportional to $\\frac{1}{i}$  \n",
    "$cf_{i}\\propto\\frac{1}{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2: Feature Represenation (10 points)\n",
    "\n",
    "In this part you will build feature vectors for each review. This will be input to our ML classifiers. You should call your parser from earlier, using all the same assumptions (e.g., casefolding, stemming). Each feature value should be the term count for that review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 57334)\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# vectors = vectorizer.fit_transform(yelp_parser(reviews[0]))\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "parsed_data = yelp_reviews_parser() \n",
    "#Overriding the internal tokenizer and pre-processor of the CountVectorizer to use my own parsed tokens directly\n",
    "vectorizer = CountVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x) \n",
    "vectors = vectorizer.fit_transform(parsed_data)\n",
    "print(vectors.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.3: Machine Learning Basics (30 points)\n",
    "\n",
    "In this part you will evaluate a bunch of classifiers -- kNN, Decision tree, Naive Bayes, and SVM -- on the feature vectors generated in the previous task in two different settings. **You do not need to implement any classifier from scratch. You may use scikit-learn's built-in capabilities.**\n",
    "\n",
    "### Setting 1: Splitting data into train-test \n",
    "\n",
    "In the first setting, you should treat the first 70% of your data as training. The remaining 30% should be for testing. \n",
    "\n",
    "### Setting 2: Using 5 fold cross-validation\n",
    "\n",
    "In the second setting, use 5-folk cross-validation. \n",
    "\n",
    "### What to report\n",
    "\n",
    "* Report the overall accuracy for both settings.\n",
    "* For the class \"Food-relevant\", report the precision and recall for both settings.\n",
    "* For the class \"Food-irrelevant\", report the precision and recall for both settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********Setting 1*********\n",
      "\n",
      " ###kNN Classifier###\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  Food-relevant       0.81      0.62      0.70      4954\n",
      "Food-irrelevant       0.70      0.86      0.77      5046\n",
      "\n",
      "    avg / total       0.75      0.74      0.74     10000\n",
      "\n",
      "Accuracy-0.7394\n",
      " ###SVM Classifier###\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  Food-relevant       0.94      0.94      0.94      5952\n",
      "Food-irrelevant       0.94      0.94      0.94      6048\n",
      "\n",
      "    avg / total       0.94      0.94      0.94     12000\n",
      "\n",
      "Accuracy-0.939583333333\n",
      "\n",
      " ###Decision Tree Classifier###\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  Food-relevant       0.88      0.89      0.88      5952\n",
      "Food-irrelevant       0.89      0.88      0.89      6048\n",
      "\n",
      "    avg / total       0.89      0.89      0.89     12000\n",
      "\n",
      "Accuracy-0.885416666667\n",
      "\n",
      " ###Multinomial Naive Bayes Classfier###\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  Food-relevant       0.95      0.94      0.95      5952\n",
      "Food-irrelevant       0.94      0.95      0.95      6048\n",
      "\n",
      "    avg / total       0.95      0.95      0.95     12000\n",
      "\n",
      "Accuracy-0.945916666667\n"
     ]
    }
   ],
   "source": [
    "# your code here...plus add cells for reporting your results\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "fr_labels = data['label'].map(lambda x: 1 if x == 'Food-relevant' else 0).values\n",
    "# print encoded_labels\n",
    "\n",
    "review_train, review_test, label_train, label_test = train_test_split(vectors, fr_labels, test_size=0.3, random_state=1337)\n",
    "# print(review_train.shape, review_test.shape)\n",
    "\n",
    "print \"\\n\\n********Setting 1*********\"\n",
    "print \"\\n ###kNN Classifier (Prediction on 10000 due to memory constraints)###\"\n",
    "knnclassifier = KNeighborsClassifier()\n",
    "knnclassifier.fit(review_train, label_train)\n",
    "predictions = knnclassifier.predict(review_test[:10000])\n",
    "print metrics.classification_report(label_test[:10000],predictions,target_names=['Food-relevant','Food-irrelevant'])\n",
    "# print \"Precision-\"+str(metrics.precision_score(label_test, predictions))\n",
    "# print \"Recall-\"+str(metrics.recall_score(label_test, predictions))\n",
    "print \"Accuracy-\"+str(metrics.accuracy_score(label_test[:10000], predictions))\n",
    "print \" ###SVM Classifier###\"\n",
    "svmclassifier = LinearSVC()\n",
    "svmclassifier.fit(review_train, label_train)\n",
    "predictions = svmclassifier.predict(review_test)\n",
    "print metrics.classification_report(label_test,predictions,target_names=['Food-relevant','Food-irrelevant'])\n",
    "# print \"Precision-\"+str(metrics.precision_score(label_test, predictions))\n",
    "# print \"Recall-\"+str(metrics.recall_score(label_test, predictions))\n",
    "print \"Accuracy-\"+str(metrics.accuracy_score(label_test, predictions))\n",
    "print \"\\n ###Decision Tree Classifier###\"\n",
    "dtclassifier = DecisionTreeClassifier()\n",
    "dtclassifier.fit(review_train, label_train)\n",
    "predictions = dtclassifier.predict(review_test)\n",
    "print metrics.classification_report(label_test,predictions,target_names=['Food-relevant','Food-irrelevant'])\n",
    "# print \"Precision-\"+str(metrics.precision_score(label_test, predictions))\n",
    "# print \"Recall-\"+str(metrics.recall_score(label_test, predictions))\n",
    "print \"Accuracy-\"+str(metrics.accuracy_score(label_test, predictions))\n",
    "print \"\\n ###Multinomial Naive Bayes Classfier###\"\n",
    "nbclassifier = MultinomialNB()\n",
    "nbclassifier.fit(review_train, label_train)\n",
    "predictions = nbclassifier.predict(review_test)\n",
    "print metrics.classification_report(label_test,predictions,target_names=['Food-relevant','Food-irrelevant'])\n",
    "# print \"Precision-\"+str(metrics.precision_score(label_test, predictions))\n",
    "# print \"Recall-\"+str(metrics.recall_score(label_test, predictions))\n",
    "print \"Accuracy-\"+str(metrics.accuracy_score(label_test, predictions))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Setting 2*********\n",
      "\n",
      " ###kNN Classifier###\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  Food-relevant       0.76      0.39      0.52     12000\n",
      "Food-irrelevant       0.72      0.93      0.81     20000\n",
      "\n",
      "    avg / total       0.73      0.73      0.70     32000\n",
      "\n",
      "Accuracy-0.7349375\n",
      " ###SVM Classifier###\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  Food-relevant       0.88      0.87      0.87     12000\n",
      "Food-irrelevant       0.92      0.93      0.92     20000\n",
      "\n",
      "    avg / total       0.91      0.91      0.91     32000\n",
      "\n",
      "Accuracy-0.9106875\n",
      "\n",
      " ###Decision Tree Classifier###\n",
      "(32000,)\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  Food-relevant       0.80      0.80      0.80     12000\n",
      "Food-irrelevant       0.88      0.88      0.88     20000\n",
      "\n",
      "    avg / total       0.85      0.85      0.85     32000\n",
      "\n",
      "Accuracy-0.8603125\n",
      "\n",
      " ###Multinomial Naive Bayes Classfier###\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  Food-relevant       0.91      0.90      0.91     12000\n",
      "Food-irrelevant       0.94      0.95      0.94     20000\n",
      "\n",
      "    avg / total       0.93      0.93      0.93     32000\n",
      "\n",
      "Accuracy-0.9325625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=4, shuffle=False) \n",
    "kf.get_n_splits(vectors)\n",
    "# print(kf) \n",
    "\n",
    "for train_index, test_index in kf.split(vectors):\n",
    "    review_train, review_test = vectors[train_index], vectors[test_index]\n",
    "    label_train, label_test = fr_labels[train_index], fr_labels[test_index]\n",
    "\n",
    "\n",
    "# print(review_train.shape, review_test.shape)\n",
    "\n",
    "print \"********Setting 2*********\"\n",
    "print \"\\n ###kNN Classifier###\"\n",
    "knnclassifier = KNeighborsClassifier()\n",
    "knnclassifier.fit(review_train, label_train)\n",
    "predictions = cross_val_predict(knnclassifier,review_train,label_train)\n",
    "print metrics.classification_report(label_train,predictions,target_names=['Food-relevant','Food-irrelevant'])\n",
    "# print \"Precision-\"+str(np.mean(cross_val_score(knnclassifier, review_train, label_train, cv=5, scoring='precision')))\n",
    "# print \"Recall-\"+str(np.mean(cross_val_score(knnclassifier, review_train, label_train, cv=5, scoring='recall')))\n",
    "print \"Accuracy-\"+str(np.mean(cross_val_score(knnclassifier, review_train, label_train, cv=5)))\n",
    "\n",
    "print \" ###SVM Classifier###\"\n",
    "svmclassifier = LinearSVC()\n",
    "svmclassifier.fit(review_train, label_train)\n",
    "predictions = cross_val_predict(svmclassifier,review_train,label_train)\n",
    "print metrics.classification_report(label_train,predictions,target_names=['Food-relevant','Food-irrelevant'])\n",
    "# print \"Precision-\"+str(np.mean(cross_val_score(svmclassifier, review_train, label_train, cv=5, scoring='precision')))\n",
    "# print \"Recall-\"+str(np.mean(cross_val_score(svmclassifier, review_train, label_train, cv=5, scoring='recall')))\n",
    "print \"Accuracy-\"+str(np.mean(cross_val_score(svmclassifier, review_train, label_train, cv=5)))\n",
    "\n",
    "print \"\\n ###Decision Tree Classifier###\"\n",
    "dtclassifier = DecisionTreeClassifier()\n",
    "dtclassifier.fit(review_train, label_train)\n",
    "predictions = cross_val_predict(dtclassifier,review_train,label_train)\n",
    "print predictions.shape\n",
    "print metrics.classification_report(label_train,predictions,target_names=['Food-relevant','Food-irrelevant'])\n",
    "\n",
    "# print \"Precision-\"+str(np.mean(cross_val_score(dtclassifier, review_train, label_train, cv=5, scoring='precision')))\n",
    "# print \"Recall-\"+str(np.mean(cross_val_score(dtclassifier, review_train, label_train, cv=5, scoring='recall')))\n",
    "print \"Accuracy-\"+str(np.mean(cross_val_score(dtclassifier, review_train, label_train, cv=5)))\n",
    "\n",
    "print \"\\n ###Multinomial Naive Bayes Classfier###\"\n",
    "nbclassifier = MultinomialNB()\n",
    "nbclassifier.fit(review_train, label_train)\n",
    "predictions = cross_val_predict(nbclassifier,review_train,label_train)\n",
    "print metrics.classification_report(label_train,predictions,target_names=['Food-relevant','Food-irrelevant'])\n",
    "# print \"Precision-\"+str(np.mean(cross_val_score(nbclassifier, review_train, label_train, cv=5, scoring='precision')))\n",
    "# print \"Recall-\"+str(np.mean(cross_val_score(nbclassifier, review_train, label_train, cv=5, scoring='recall')))\n",
    "print \"Accuracy-\"+str(np.mean(cross_val_score(nbclassifier, review_train, label_train, cv=5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.4: Analyzing your results (5 points) \n",
    "\n",
    "OK, now that you have tried four different classifiers, what do you observe? Any conclusions you can draw? Give us one or two paragraphs summarizing your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*add your discussion here*\n",
    "\n",
    "According to the classification scores obtained: **Multinomial Naive Bayes** emerges as the best classifier for this dataset, followed closely by **SVM Classifier (using a Linear kernel)**. The **Decision tree classifier** performs well. However, the **kNN Classifier** performs poorly.\n",
    "\n",
    "**Precision** - the fraction of retrieved documents that are relevant\n",
    "$$Precision =\\frac{TP}{TP + FP}$$\n",
    "**Recall** - the fraction of relevant documents that are retrieved \n",
    "$$Recall =\\frac{TP}{TP + FN}$$\n",
    "**Accuracy** -  the fraction of classifications that are correct\n",
    "$$Accuracy =\\frac{TP+TN}{TP + FN + FP + TN}$$\n",
    "#### Multinomial Naive Bayes \n",
    "As the Multinomial Naive Bayes variation of the Naive Bayes algorithm is helpful when multiple occurrences of the words is important in the classification problem, it performs the best on this dataset. Also, the multinomial classifier naturally handles texts of varying length by incorporating the conditional probability of a particular word for a class by computing the relative frequency of term in documents belonging to that class (incldung multiple occurrences). On the other hand, it makes the assumption of conditional independence of the features which is not likely true for this dataset (as the words in the reviews are obviously related in terms of food choices, restaurants etc.) \n",
    "#### SVM Classifier\n",
    "Even though, the Linear SVM classifier is widely regarded as one of the best text classfication algorithm it falls short of the performance achieved by MNB and is slightly slow when compared to it. The data in this problem is linearly separable aiding linear SVM to performs so well.Also, due to a high dimensional feature space, sparse document vectors and few irrelevant features, SVM thrives in this problem as it does not need aggressive feature selection.\n",
    "#### Decision Tree Classifier\n",
    "Decision trees are easy to interpret and generate predcitions fast. Near the top of the decision tree it is possible for the learning algorithm to find very useful features. However, most likely in this case since the stop words were not removed, they may have appeared at the top of the tree leading to inaccurate predictions. \n",
    "Decision trees force features to be checked in a specific order, even when features may be independent of one another, which leads to repetition of these features in the lower branches of the tree(due to space limitation at the top of the tree.) This is overcome by Naive Bayes which may be the reason for its higher accuracy.  \n",
    "#### kNN Classifier\n",
    "Classifiers like Bayesian, Support Vector Machine and Decision Tree are \"eager learners\", because they first build a classification model on the training dataset before actually classifying a sample from the test dataset. However, kNN is a \"lazy classifier\" and directly learns from the training instances, processing data only after it is given a test observation to classify. It requires frequent database lookups and therefore, is computationally expensive. kNN is also very sensitive to bad features and thus, feature selection is neccessary (which was not performed). kNN is sensitive to outliers and for a higher dimensional space approximation is used (which was not performed). These problems led to a poor performance on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.5: Improving your classifier (10 points)\n",
    "\n",
    "I think we can do better! In this part, your job is to create new features that you can think can help improve your classifier. You may choose to use new weightings for your words, new derived features (e.g., count of 3-letter words), or whatever you like. You may also add in the extra features in the json: funny, useful, cool. You will need to experiment with different approaches ... once you finalize on your best approach, include the features here with a description (that is, tell us what the feature means). Then give us your classifier results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features/Techniques that can help improve the classifier\n",
    "#### Removal of stopwords\n",
    "The most commmon words in the language(here English) like 'the', 'and' etc. can be removed as they take up space and eat up valuable processing time without adding any value to the classification. This helped in bringing the feature space size down significantly and improving the classification scores.\n",
    "In this part, I used the default \"english\" stop list provided in sklearn's TfidfVectorizer function by specifying the \"stop_words\" parameter. \n",
    "#### Using  Term Frequency–Inverse Document Frequency instead of Term Counts\n",
    "After removing the stopwords, I used the feature extractor TfidfVectorizer to transform th review text to feature vectors. Tf-idf is one of the most popularly used term-weighing schemes in information retrieval and is effective in emphasizing the most important words for a given document. The Tf-idf value is a combination of the term frequency and inverse document frequency. The tf-idf value increases proportionally to the number of times a word appears in the document. However, this is offset by the frequency of the word in the vocabulary taking into account the fact that some words appear more frequently in general. Tuning parameters such as min_df (which ignores terms that have a frequency strictly lower than the given threshold) also helps in improving the classification results. \n",
    "#### Adding bigrams and trigrams\n",
    "Using the TfidfVectorizer parameter \"n_gramrange\", I extracted bigrams(2 words occuring frequently together) and trigrams (3 words occuring frequently together) leading to better classficiation scores and accuracy as higher order ngrams usually carry more information about the context in general which help in classification easily. \n",
    "#### Tuning the hyperparameters for a classifier\n",
    "Tuning hyperparameters specific to a classifier can also help in improving the classification results.\n",
    "#### Adding Parts of Speech Tagging\n",
    "Using a Parts of Speech(POS) tagger (labelling words with their appropriate Part-Of-Speech like Noun, Verb, Adjective) can also help in improving the classification results. This technique was not explored due to time constraints.\n",
    "#### Augmenting the vocabulary with higher weights for Food-related words\n",
    "Assigning higher weights to words related to food as seen in the most informative features section can help in further improving the results. This technique was not explored due to time constraints.\n",
    "#### Feature Selection\n",
    "Most informative features can be selected through univariate feature selection, chi2 and f-value tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 39216)\n",
      "\n",
      "\n",
      "********Setting 1*********\n",
      "\n",
      " ###kNN Classifier (Prediction on 10000 due to memory constraints)###\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  Food-relevant       0.51      0.19      0.28      4954\n",
      "Food-irrelevant       0.51      0.82      0.63      5046\n",
      "\n",
      "    avg / total       0.51      0.51      0.45     10000\n",
      "\n",
      "Accuracy-0.5084\n",
      " ###SVM Classifier###\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  Food-relevant       0.95      0.96      0.96      5952\n",
      "Food-irrelevant       0.96      0.95      0.96      6048\n",
      "\n",
      "    avg / total       0.96      0.96      0.96     12000\n",
      "\n",
      "Accuracy-0.9575\n",
      "\n",
      " ###Decision Tree Classifier###\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  Food-relevant       0.90      0.89      0.90      5952\n",
      "Food-irrelevant       0.89      0.90      0.90      6048\n",
      "\n",
      "    avg / total       0.90      0.90      0.90     12000\n",
      "\n",
      "Accuracy-0.896583333333\n",
      "\n",
      " ###Multinomial Naive Bayes Classfier###\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  Food-relevant       0.95      0.96      0.95      5952\n",
      "Food-irrelevant       0.96      0.95      0.95      6048\n",
      "\n",
      "    avg / total       0.95      0.95      0.95     12000\n",
      "\n",
      "Accuracy-0.95075\n"
     ]
    }
   ],
   "source": [
    "# your code here ... add as many cells as you need for features, results, and discussion.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "# parsed_data = yelp_reviews_parser() \n",
    "encoded_labels = data['label'].map(lambda x: 1 if x == 'Food-relevant' else 0).values\n",
    "\n",
    "vectorizer = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, ngram_range=(1,3), min_df=10, stop_words=\"english\")\n",
    "vectors = vectorizer.fit_transform(parsed_data)\n",
    "print(vectors.shape)\n",
    "review_train, review_test, label_train, label_test = train_test_split(vectors, encoded_labels, test_size=0.3, random_state=1337)\n",
    "# print(review_train.shape, review_test.shape)\n",
    "\n",
    "print \"\\n\\n********Setting 1*********\"\n",
    "print \"\\n ###kNN Classifier (Prediction on 10000 due to memory constraints)###\"\n",
    "knnclassifier = KNeighborsClassifier()\n",
    "knnclassifier.fit(review_train, label_train)\n",
    "predictions = knnclassifier.predict(review_test[:10000])\n",
    "print metrics.classification_report(label_test[:10000],predictions,target_names=['Food-relevant','Food-irrelevant'])\n",
    "print \"Accuracy-\"+str(metrics.accuracy_score(label_test[:10000], predictions))\n",
    "print \" ###SVM Classifier###\"\n",
    "svmclassifier = LinearSVC()\n",
    "svmclassifier.fit(review_train, label_train)\n",
    "predictions = svmclassifier.predict(review_test)\n",
    "print metrics.classification_report(label_test,predictions,target_names=['Food-relevant','Food-irrelevant'])\n",
    "print \"Accuracy-\"+str(metrics.accuracy_score(label_test, predictions))\n",
    "print \"\\n ###Decision Tree Classifier###\"\n",
    "dtclassifier = DecisionTreeClassifier()\n",
    "dtclassifier.fit(review_train, label_train)\n",
    "predictions = dtclassifier.predict(review_test)\n",
    "print metrics.classification_report(label_test,predictions,target_names=['Food-relevant','Food-irrelevant'])\n",
    "print \"Accuracy-\"+str(metrics.accuracy_score(label_test, predictions))\n",
    "print \"\\n ###Multinomial Naive Bayes Classfier###\"\n",
    "nbclassifier = MultinomialNB()\n",
    "nbclassifier.fit(review_train, label_train)\n",
    "predictions = nbclassifier.predict(review_test)\n",
    "print metrics.classification_report(label_test,predictions,target_names=['Food-relevant','Food-irrelevant'])\n",
    "print \"Accuracy-\"+str(metrics.accuracy_score(label_test, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS: What are the most informative features in distinguishing these two classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most informative features according to Chi-squared test \n",
      "['airport', 'car', 'chees', 'chicken', 'delici', 'dish', 'dr', 'eat', 'food', 'fri', 'lunch', 'meal', 'menu', 'order', 'pizza', 'restaur', 'salad', 'sandwich', 'sauc', 'wine']\n",
      "Most informative features according to ANOVA F-value test \n",
      "['chees', 'chicken', 'delici', 'dish', 'eat', 'flavor', 'food', 'fresh', 'fri', 'good', 'help', 'lunch', 'meal', 'menu', 'order', 'restaur', 'salad', 'sandwich', 'sauc', 'tast']\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "#Selecting the 20 most informative features\n",
    "\n",
    "ch2 = SelectKBest(chi2, k=20)\n",
    "\n",
    "train_data = ch2.fit_transform(review_train, label_train)\n",
    "test_data = ch2.transform(review_test)\n",
    "informative_feature_names = [feature_names[i] for i in ch2.get_support(indices=True)]\n",
    "print \"Most informative features according to Chi-squared test \"\n",
    "print informative_feature_names\n",
    "\n",
    "anova = SelectKBest(f_classif, k=20)\n",
    "\n",
    "train_data = anova.fit_transform(review_train, label_train)\n",
    "test_data = anova.transform(review_test)\n",
    "informative_feature_names = [feature_names[i] for i in anova.get_support(indices=True)]\n",
    "print \"Most informative features according to ANOVA F-value test \"\n",
    "print informative_feature_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Learning to Rank (30 points)\n",
    "\n",
    "For this part, we're going to play with some Microsoft LETOR data that has query-document relevance judgments. Let's see how learning to rank works in practice. \n",
    "\n",
    "First, you will need to download the MQ2008.zip file from the Resources tab on Piazza. This is data from the [Microsoft Research IR Group](https://www.microsoft.com/en-us/research/project/letor-learning-rank-information-retrieval/).\n",
    "\n",
    "The data includes 15,211 rows. Each row is a query-document pair. The first column is a relevance label of this pair (0,1 or 2--> the higher value the more related), the second column is query id, the following columns are features, and the end of the row is comment about the pair, including id of the document. A query-document pair is represented by a 46-dimensional feature vector. Features are a numeric value describing a document and query such as TFIDF, BM25, Page Rank, .... You can find compelete description of features from [here](https://arxiv.org/ftp/arxiv/papers/1306/1306.2597.pdf).\n",
    "\n",
    "The good news for you is the dataset is ready for analysis: It has already been split into 5 folds (see the five folders called Fold1, ..., Fold5).\n",
    "\n",
    "For this assignment, we're going to leave our favorite scikit-learn and instead use [SVM-rank](https://www.cs.cornell.edu/people/tj/svm_light/svm_rank.html). This is the basic ranking SVM we talked about in class. You'll see that SVM-rank considers pairwise relevance between docs -- so based on the training data it will transform the data into pairs -- like D1 > D2 and then learn a separator.\n",
    "\n",
    "\n",
    "## Part 2.1: Optimizing SVM-Rank (15 points)\n",
    "\n",
    "First, you should explore how the different parameters affect the quality of the Ranking SVM. You'll see that you can vary the kernel function, the loss function and so forth. \n",
    "\n",
    "You should run SVM-Rank using the default options over each of the five folds. You should find the error on the test set (for example, depending on your settings, svm_rank_classify will give you the zero/one error statistics (that is, the number of correct pairs and the number of incorrect pairs). Report the average. \n",
    "\n",
    "Then try different parameters and report how they impact the quality of results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*add your results and discussion here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Results and Discussion\n",
    "#### The following results were obtained by changing the parameters:\n",
    "#### Configuration 1 (c = 20.0)\n",
    "Zero/one-error on test set (average across the 5 folds) = 61.522\n",
    "#### Configuration 2 (c = 20.0, loss function - 1)\n",
    "Zero/one-error on test set (average across the 5 folds) = 61.522\n",
    "#### Configuration 3 (c = 20.0, loss function - 2)\n",
    "Zero/one-error on test set (average across the 5 folds) = 59.992\n",
    "#### Configuration 4 (c = 20.0, w - 4, f - 10)\n",
    "Zero/one-error on test set (average across the 5 folds) = 61.436\n",
    "#### Configuration 5 (c = 10.0)\n",
    "Zero/one-error on test set (average across the 5 folds) = 61.522\n",
    "#### Configuration 6 (c = 100.0)\n",
    "Zero/one-error on test set (average across the 5 folds) = 61.236\n",
    "#### Configuration 7 (c = 1000.0)\n",
    "Zero/one-error on test set (average across the 5 folds) = 61.436\n",
    "#### Configuration 8 (c = 20.0, w - 2, f - 10)\n",
    "Zero/one-error on test set (average across the 5 folds) = 61.35\n",
    "#### Configuration 9 (c = 20.0, e - 0.1)\n",
    "Zero/one-error on test set (average across the 5 folds) = 61.66\n",
    "\n",
    "#### The kernel trick parameters were extremely slow for me (I left one overnight!!!). I tried experimenting with the gamma value for RBF kernel and d parameter for polynomial kernel. However, they were extremely time consuming (as mentioned on the SVM rank documentation page itself). I even tried SVM light to overcome this and the kernel trick options were working for that but I could not figure out a way to get it to report the zero/one error.\n",
    "### Observations\n",
    "#### C Parameter\n",
    "Here C is a learning parameter and is one of the most important hyperparameters for SVM. It is used to adjust the trade off between training error and low testing error. Thus, a smaller C value (default-0.01) increases the number of training errors as the optimizer looks for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points and a large C value the optimization will choose a smaller-margin hyperplane does a better job of getting all the training points classified correctly.\n",
    "#### epsilon Parameter\n",
    "This parameter controls the tolerance for termination criterion.\n",
    "The larger the value is, the more errors are admitted in the solution.\n",
    "#### The linear SVM kernel was computationally less intensive as compared to the other kernels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2.1: Noise! (15 points)\n",
    "\n",
    "Now we're going to investigate whether the ranking SVM is easily influenced by noisy features. For example, what if some of the features you have are in error? Or what if you downloaded only a portion of a page to calculate a feature? (so the count of inlinks would be wrong)? \n",
    "\n",
    "In this case, add some noise to the features. What happens to the results? You may choose to add random noise throughout, noise to a single feature, noise to multiple features, etc. The choices are up to you. We aim to see what kind of exploration you conduct and what you conclude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*add your results and discussion here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Results and Discussion\n",
    "I created a python script(added below) to add noise (random, multiple features[Features 10|19|21|25|32|37|43] and single feature[26]) to the dataset  \n",
    "#### The following configurations were tested after adding noise:\n",
    "#### Configuration 1 (c = 20.0)\n",
    "Zero/one-error on test set (average across the 5 folds) = 62.158(random noise), 61.604(multiple feature noise), 61.522(single feature noise)\n",
    "#### Configuration 2 (c = 20.0, loss function - 1)\n",
    "Zero/one-error on test set (average across the 5 folds) = 62.158(random noise), 61.604(multiple feature noise), 61.522(single feature noise)\n",
    "#### Configuration 3 (c = 20.0, loss function - 2)\n",
    "Zero/one-error on test set (average across the 5 folds) = 61.222(random noise), 60.586(multiple feature noise), 60.16(single feature noise)\n",
    "#### Configuration 4 (c = 20.0, w - 4, f - 10)\n",
    "Zero/one-error on test set (average across the 5 folds) = 62.288(random noise), 61.47(multiple feature noise), 61.308(single feature noise)\n",
    "\n",
    "I was not able to detect signifcant changes in the results after adding noise to the dataset indicating that SVM is fairly robust and capable of handling noise in the dataset. Though, in case of random noise the error was higher than multiple feature noise and single feature noise. This was as expected. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "def random_digits():\n",
    "    rand_str = ':0.'\n",
    "    rand_str = rand_str + ''.join(random.choice(string.digits) for x in range(6))\n",
    "    return rand_str\n",
    "def m_replace(m):\n",
    "    if m:\n",
    "        l = m.group(0).split(':')\n",
    "        l[1] = str(random_digits())\n",
    "        return ''.join(l)\n",
    "\n",
    "re1='(\\\\d)+'\t# Any Single Digit 1\n",
    "re2='(:)'\t# Any Single Character 1\n",
    "re3='([+-]?\\\\d*\\\\.\\\\d+)(?![-+0-9\\\\.])'\t# Float 1\n",
    "folds = ['Fold1','Fold2','Fold3','Fold4','Fold5']\n",
    "for fold in folds:\n",
    "    file = fold+\"/train.txt\"\n",
    "    outfile = fold+\"/train1.txt\"\n",
    "    if sys.argv[1] == 'random':\n",
    "        rg = re.compile(re2+re3,re.IGNORECASE|re.DOTALL)\n",
    "        with open(outfile, \"w\") as target:\n",
    "            with open(file, \"r\") as source:\n",
    "                    for line in source:\n",
    "                        if random.choice([0,1]):\n",
    "                            rand = str(random_digits())\n",
    "                            line = rg.sub(rand,line,int(random.choice(string.digits)))\n",
    "                        target.write(line)\n",
    "    elif sys.argv[1] == 'multiple':\n",
    "        re1 = '(10|19|21|25|32|37|43)'\n",
    "        rg = re.compile(re1+re2+re3,re.IGNORECASE|re.DOTALL)\n",
    "        with open(outfile, \"w\") as target:\n",
    "            with open(file, \"r\") as source:\n",
    "                    for line in source:\n",
    "                        rand = str(random_digits())\n",
    "                        line = rg.sub(m_replace,line)\n",
    "                        target.write(line)\n",
    "    elif sys.argv[1] == 'single':\n",
    "        re1 = '(26)'\n",
    "        rg = re.compile(re1+re2+re3,re.IGNORECASE|re.DOTALL)\n",
    "        with open(outfile, \"w\") as target:\n",
    "            with open(file, \"r\") as source:\n",
    "                    for line in source:\n",
    "                        rand = str(random_digits())\n",
    "                        line = rg.sub(m_replace,line)\n",
    "                        target.write(line)\n",
    "    else:\n",
    "        print \"Add some arguments for the ype of noise - random, multiple, single\"                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaboration declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you collaborated with anyone (see Collaboration policy at the top of this homework), you can put your collaboration declarations here.*\n",
    "I consulted the following sources for this homework:\n",
    "Piazza, sklearn documentation \n",
    "\n",
    "https://stackoverflow.com/questions/35867484/pass-tokens-to-countvectorizer\n",
    "\n",
    "https://stackoverflow.com/questions/31175140/get-a-classification-report-stating-the-class-wise-precision-and-recall-for-mult\n",
    "\n",
    "http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "\n",
    "https://de.dariah.eu/tatom/preprocessing.html\n",
    "\n",
    "https://medium.com/@aneesha/visualising-top-features-in-linear-svm-with-scikit-learn-and-matplotlib-3454ab18a14d\n",
    "\n",
    "https://medium.com/tensorist/classifying-yelp-reviews-using-nltk-and-scikit-learn-c58e71e962d9\n",
    "\n",
    "https://de.dariah.eu/tatom/preprocessing.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
